---
title: "simulation_prs_linear"
author: "Tobias Gradinger"
date: "2024-07-22"
---

```{r Setup, include = F}
knitr::opts_chunk$set(include = F)
```

```{r Packages}
library(dplyr)
library(tidyr)
library(readr)
library(magrittr)
library(ggplot2)
library(pander)
library(ppcor)
library(car)
library(ggpubr)
library(gridExtra)
library(cowplot)
library(patchwork)
library(conflicted)
library(QuantPsyc)
library(MeMoBootR)
library(epitools)
#library(png)
```

```{r SETUP clean-up, eval=T}
# remove all objects to start with clean environment
rm(list=ls())
```

```{r SETUP solve function conflicts}
conflicts_prefer(dplyr::filter, dplyr::select, dplyr::n())
```

```{r Functions}
t.report <- function(tt){
  tvalue <- tt$statistic %>% formatC(digits = 2, format = "f")
  pvalue <- tt$p.value %>% formatC(digits = 2, format = "f")
  if (round(tt$parameter, 0) == tt$parameter) {
    df <- tt$parameter
  } else {
    df <- formatC(digits = 2, format = "f")
  }
  if (tt$p.value < 0.0005) {
    pvalue <- " < 0.001" 
  } else { 
    if (tt$p.value < 0.005) {
      pvalue <- paste0(" = ",tt$p.value %>% formatC(digits = 3, format = "f"))
    } else {
      pvalue <- paste0(" = ",tt$p.value %>% formatC(digits = 2, format = "f"))
    }
    } 
  paste0("*t*(",df,") = ",tvalue, ", *p*", pvalue)
}

mean_sd_se.report <- function(df, filtervar, filter, var){
  mean <- df %>% filter (!!filtervar == filter) %>% .[[var]] %>% mean() %>% round(2)
  sd <- df %>% filter (!!filtervar == filter) %>% .[[var]] %>% sd() %>% round(2)
  se <- df %>% filter (!!filtervar == filter) %>% .[[var]] %>% se() %>% round(2)
  
  paste0(filter," mean = ", mean, ", sd = ", sd, ", ", "se = ", se)
}

remove_outliers <- function(data, variables, multiplier = 2) {
  for (variable in variables) {
    Q1 <- quantile(data[[variable]], 0.25, na.rm = TRUE)
    Q3 <- quantile(data[[variable]], 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - multiplier * IQR
    upper_bound <- Q3 + multiplier * IQR

    data <- data %>%
      filter(get(variable) >= lower_bound & get(variable) <= upper_bound)
  }
  
  return(data)
}
```

```{r Simulate prob(y=1) = x*constant}
set.seed(1)
# Generate normally distributed x with mean 50 and standard deviation 10
n <- 427350

x <- round(rnorm(n, mean = 190, sd = 8.5))

# Calculate linear probabilities with a custom scaling to achieve ~4% overall probability of y = 1
prob_y <- (x - min(x)) / (max(x) - min(x)) * 0.096

# Sample y based on the calculated probabilities
y <- rbinom(n, size = 1, prob = prob_y)

# Create a data frame
simulated_data <- data.frame(x = x, y = y, prob_y = prob_y)

# Verify the overall percentage of y = 1
mean(y)
sum(y)
```

```{r Alternative Simulation with logit as link function - does not work as desired yet}
set.seed(123)
n <- 427350
x <- round(rnorm(n, mean = 190, sd = 8.5))

x_scaled <- (x - min(x)) / (max(x) - min(x))

logit <- function(x) {
  log(x/(1-x))
}

logit_x <- logit(x_scaled)

prob_y <- exp(logit_x)/ (1+exp(logit_x))

plot(x, prob_y)

logit_x_scaled <- (logit_x - min(logit_x)) / (max(logit_x) - min(logit(x)))

plot(x,logit_x)

plot(x, logit_x_scaled)

```

```{r graphics to check properties of simulated data}
plot_prob_y_x <- ggplot(simulated_data, aes(x = x, y = prob_y)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red", size = 0.7) +
  labs(title = "Probability of y = 1 vs. x",
       x = "x",
       y = "Probability of y = 1") +
  theme_minimal() +
  theme(axis.title = element_text(face = "bold", size = 10),
        axis.text = element_text(size = 10),
        plot.title = element_text(hjust = 0.5))

# Display the plot
plot_prob_y_x
hist(x)
max(x)
min(x)
print(mean(y))
```

```{r reproducing the table from page 84 of the supplements from the pang schunkert paper with the simulated data}
# This shows that the simulated data has very similar properties to the data from the paper

supplemental_page_84 <-
simulated_data %>%
  mutate(decile = ntile(x,10)) %>%
  group_by(decile) %>%
  summarize(total_cases= sum(y),
            total_controls = length(y) - sum(y),
            Risk_Allels_mean = mean(x),
            Risk_Allels_sd = sd(x),
            prevalence = mean(y) * 100) %>% 
  mutate(allele_increase_perc = (Risk_Allels_mean/first(Risk_Allels_mean)-1)*100,
         prevalence_increase_perc = (prevalence/first(prevalence)-1)*100)


# Print quintile summary
print(supplemental_page_84)
```

```{r plot to show the characteristis fo a decile plot}
scaling_factor <- 0.04

p_combined <- ggplot(supplemental_page_84, aes(x = factor(decile))) +
  geom_point(aes(y = prevalence), color = "skyblue", size = 3) +
  geom_line(aes(y = prevalence, group = 1), color = "skyblue", size = 1) +
  geom_point(aes(y = Risk_Allels_mean*scaling_factor), color = "red", size = 3) +
  geom_line(aes(y = Risk_Allels_mean*scaling_factor, group = 1), color = "red", size = 1) +
  geom_errorbar(aes(ymin = (Risk_Allels_mean*scaling_factor - Risk_Allels_sd*scaling_factor), ymax = (Risk_Allels_mean*scaling_factor + Risk_Allels_sd*scaling_factor)), color = "darkred", width = 0.2) +
  labs(title = "Percentage of y = 1 and Mean of x for Each Quintile",
       x = "Decile",
       y = "Percentage of y = 1 / Mean of x") +
  theme_minimal() +
  scale_y_continuous(
    name = "Prevalence in %",
    sec.axis = sec_axis(~ ., name = "Mean of x scaled")
  ) +
  theme(axis.title.y.right = element_text(color = "red"),
        plot.background = element_rect(fill = "white"),  # Set the plot background to white
        panel.background = element_rect(fill = "white")
  )

# Display the plot
print(p_combined)

ggsave("graphics/decile_prevalence.png", p_combined)
```

```{r split the data into groups of x like in the paper}
data.splits.x <-
  simulated_data %>% 
  group_by(x) %>%
  summarize(prev_y = mean(y),
            sum_y = sum(y),
            n=n()) %>%
  filter(n>=200) %>% 
  mutate(prev_se = sqrt(prev_y*(1-prev_y)/n))
```

```{r Rebuilding figure2 from Schunkert paper, fig.width=4, fig.height=6}
# Calculate the max y value plus standard error
max_y_plus_se <- max(data.splits.x$prev_y + data.splits.x$prev_se)

# Define the polygon coordinates based on the filtered data
x_coords <- c(min(supplemental_page_84$Risk_Allels_mean[2]), 
               min(supplemental_page_84$Risk_Allels_mean[2]), 
               max(supplemental_page_84$Risk_Allels_mean[9]), 
               max(supplemental_page_84$Risk_Allels_mean[9]))
y_coords <- c(0, max_y_plus_se, max_y_plus_se, 0)

# Fit models
modelLogit <- glm(cbind(sum_y, n) ~ x, data = data.splits.x, family = binomial(link = "logit"))
modelProbit <- glm(cbind(sum_y, n) ~ x, data = data.splits.x, family = binomial(link = "probit"))
modelLog <- glm(cbind(sum_y, n) ~ x, data = data.splits.x, family = binomial(link = "log"))
modelLinear <- lm(prev_y ~ x, data = data.splits.x, weights = n)

# Make predictions
data.splits.x$predLogit <- predict(modelLogit, type = "response")
data.splits.x$predProbit <- predict(modelProbit, type = "response")
data.splits.x$predLog <- predict(modelLog, type = "response")
data.splits.x$predLin <- predict(modelLinear, type = "response")

# Calculate correlations
Rlogit <- round(unlist(cor.test(data.splits.x$prev_y, data.splits.x$predLogit)[c("estimate", "conf.int")]), 2)
Rprobit <- round(unlist(cor.test(data.splits.x$prev_y, data.splits.x$predProbit)[c("estimate", "conf.int")]), 2)
Rlog <- round(unlist(cor.test(data.splits.x$prev_y, data.splits.x$predLog)[c("estimate", "conf.int")]), 2)
Rlin <- round(unlist(cor.test(data.splits.x$prev_y, data.splits.x$predLin)[c("estimate", "conf.int")]), 2)

# Create the legend text
cor_text <- paste(
  paste0("Logit: R=", Rlogit[1], " (95%CI: [", Rlogit[2], "-", Rlogit[3], "])"),
  paste0("Log: R=", Rlog[1], " (95%CI: [", Rlog[2], "-", Rlog[3], "])"),
  paste0("Probit: R=", Rprobit[1], " (95%CI: [", Rprobit[2], "-", Rprobit[3], "])"),
  paste0("Linear: R=", Rlin[1], " (95%CI: [", Rlin[2], "-", Rlin[3], "])"),
  sep = "\n"
)

# Create the plot
fig2 <-
  ggplot(data.splits.x, aes(x = x, y = prev_y)) +
  geom_point(color = "black", size = 3) +  # Plot the dots
  geom_errorbar(aes(ymin = prev_y - prev_se, ymax = prev_y + prev_se), width = 0.2, color = "lightgrey") +
  geom_polygon(
    data = data.frame(x = x_coords, y = y_coords),  # Convert to a data frame
    aes(x = x, y = y),
    fill = rgb(0.2, 0.2, 0.2, 0.2),
    color = NA
  ) +
  geom_line(aes(y = predLogit), color = "seagreen", size = 2) +  # Add logit fit line
  geom_line(aes(y = predLog), color = "coral1", size = 2, linetype = "dashed") +  # Add log fit line
  labs(x = "Mean number of risk alleles", y = "Prevalence", title = "UKB CAD") +
  #annotate("text", x = mean(range(data.splits.x$x)), y = max_y_plus_se, label = cor_text, hjust = 0.5, size = 2.5) +
  annotate(
    "label",
    x = mean(range(data.splits.x$x)), 
    y = max_y_plus_se*0.95, 
    label = cor_text,
    fill = "white",
    color = "black",
    size = 2.5,
    label.size = 0.4,
    hjust = 0.5,
    fontface = "bold"
  )+
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),  # Remove major grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    axis.line = element_line(color = "black"),  # Keep the axes lines
    plot.title = element_text(hjust = 0.5),  # Center the title
    legend.position = "none",  # Hide default legend as we have custom annotations
    plot.background = element_rect(fill = "white"),  # Set the plot background to white
    panel.background = element_rect(fill = "white")
  )

# Display the plot
print(fig2)
```

```{r save the graphic}
ggsave("graphics/pang_schunkert_fig_2.png", fig2, width = 4, height = 6)
```

```{r Code graphics from Schunkert paper}
analysis <- function(dat, pheno) {
  modComp <- summary(glm(CC ~ NRA, family = "binomial", data = dat))$coefficients
  dat$grp <- round(dat$NRA)
  #cut(dat$NRA,breaks = seq(min(dat$NRA),max(dat$NRA),by=2),include.lowest=TRUE) 
  
  # Aggregations
  prev_p <- aggregate(dat$CC, by = list(dat$grp), FUN = mean, na.rm = TRUE)
  prev_n <- aggregate(dat$CC, by = list(dat$grp), FUN = sum, na.rm = TRUE)
  prev_N <- aggregate(dat$CC, by = list(dat$grp), FUN = function(x) length(!is.na(x)))
  allele_c <- aggregate(dat$NRA, by = list(dat$grp), FUN = mean)
  
  d <- cbind.data.frame(N = prev_N$x, n = prev_n$x, p = prev_p$x, numAllele = allele_c$x)
  d$p_se <- sqrt(d$p * (1 - d$p) / d$N)
  d <- d[which(d$N >= 200),]
  d$px <- d$p / d$numAllele
  d$px_se <- d$p_se / d$numAllele
  
  # GLM models
  modelLogit <- glm(cbind(n, N) ~ numAllele, data = d, family = binomial(link = "logit"))
  modelProbit <- glm(cbind(n, N) ~ numAllele, data = d, family = binomial(link = "probit"))
  modelLog <- glm(cbind(n, N) ~ numAllele, data = d, family = binomial(link = "log"))
  modelLinear <- lm(p ~ numAllele, data = d, weights = N)
  
  predLogit <- predict(modelLogit, type = "response")
  predProbit <- predict(modelProbit, type = "response")
  predLog <- predict(modelLog, type = "response")
  predLin <- predict(modelLinear, type = "response")
  
  Rlogit <- round(unlist(cor.test(d$p, predLogit)[c("estimate", "conf.int")]), 2)
  Rprobit <- round(unlist(cor.test(d$p, predProbit)[c("estimate", "conf.int")]), 2)
  Rlog <- round(unlist(cor.test(d$p, predLog)[c("estimate", "conf.int")]), 2)
  Rlin <- round(unlist(cor.test(d$p, predLin)[c("estimate", "conf.int")]), 2)
  
  Cols <- c("seagreen", "coral1", "goldenrod", "blue")
  l <- which(d[, "p_se"] > 0)
  
  plot(d[l, "numAllele"], d[l, "p"], pch = 19, axes = FALSE, 
       ylim = c(0, max(d$p + d$p_se)), 
       xlim = c(min(d$numAllele - sd(d$numAllele)), max(d$numAllele + sd(d$numAllele))), 
       main = pheno, xlab = "Mean number of risk alleles", ylab = "Prevalence")
  axis(1)
  axis(2)
  
  for (i in 1:nrow(d)) {
    if (d[i, "p_se"] > 0) {
      arrows(d[i, "numAllele"], d[i, "p"] - d[i, "p_se"], d[i, "numAllele"], d[i, "p"] + d[i, "p_se"], 
             col = "grey", angle = 90, len = 0.05)
      arrows(d[i, "numAllele"], d[i, "p"] + d[i, "p_se"], d[i, "numAllele"], d[i, "p"] - d[i, "p_se"], 
             col = "grey", angle = 90, len = 0.05)
    }
  }
  
  matlines(d$numAllele, cbind(predLogit, predLog), col = Cols, lwd = 2)
  abline(a = modelLog$coefficients[1], b = modelLog$coefficients[2], col = 4, lty = 3)
  quant <- quantile(dat$NRA, probs = c(0.1, 0.9))
  
  # Plot polygon
  polygon(
    x = c(quant[1], quant[1], quant[2], quant[2]),
    y = c(0, max(d$p), max(d$p), 0),
    col = rgb(0.2, 0.2, 0.2, 0.2),
    border = NA
  )
  
  # Add legend
  legend(
    "topleft",
    legend = c(
      paste0("Logit: R=", Rlogit[1], " (95%CI: [", Rlogit[2], "-", Rlogit[3], "])"),
      paste0("Probit: R=", Rprobit[1], " (95%CI: [", Rprobit[2], "-", Rprobit[3], "])"),
      paste0("Linear: R=", Rlin[1], " (95%CI: [", Rlin[2], "-", Rlin[3], "])"),
      paste0("Log: R=", Rlog[1], " (95%CI: [", Rlog[2], "-", Rlog[3], "])")
    ),
    col = Cols, cex = 0.9, box.lty = 0, lwd = 2, bg = "transparent"
  )
  
  # Plot points with error bars
  plot(
    d[l, "numAllele"], d[l, "px"], pch = 19, axes = FALSE,
    ylim = c(0, max(d$px + d$px_se)),
    xlim = c(min(d$numAllele - sd(d$numAllele)), max(d$numAllele + sd(d$numAllele))),
    main = pheno, xlab = "Mean number of risk alleles", ylab = "Prevalence/Allele"
  )
  axis(1)
  axis(2)
  
  for (i in 1:nrow(d)) {
    if (d[i, "px_se"] > 0) {
      arrows(
        d[i, "numAllele"], d[i, "px"] - d[i, "px_se"],
        d[i, "numAllele"], d[i, "px"] + d[i, "px_se"],
        col = "grey", angle = 90, len = 0.05
      )
      arrows(
        d[i, "numAllele"], d[i, "px"] + d[i, "px_se"],
        d[i, "numAllele"], d[i, "px"] - d[i, "px_se"],
        col = "grey", angle = 90, len = 0.05
      )
    }
  }
  
  # Fit models
  modelLogit <- glm(px ~ numAllele, data = d, family = binomial(link = "logit"))
  modelLog <- glm(px ~ numAllele, data = d, family = binomial(link = "log"))
  
  # Predictions
  predLogit <- predict(modelLogit, type = "response")
  predLog <- predict(modelLog, type = "response")
  
  # Correlations
  Rlogit <- round(unlist(cor.test(d$p, predLogit)[c("estimate", "conf.int")]), 2)
  Rlog <- round(unlist(cor.test(d$p, predLog)[c("estimate", "conf.int")]), 2)
  
  # Plot lines
  matlines(d$numAllele, cbind(predLogit, predLog), col = Cols, lwd = 2)
  
  # Quantiles and polygon
  quant <- quantile(dat$NRA, probs = c(0.1, 0.9))
  polygon(
    x = c(quant[1], quant[1], quant[2], quant[2]),
    y = c(0, max(d$px), max(d$px), 0),
    col = rgb(0.2, 0.2, 0.2, 0.2),
    border = NA
  )
  
  # Legend
  legend(
    "topleft",
    legend = c(
      paste0("Logit: R=", Rlogit[1], " (95%CI: [", Rlogit[2], "-", Rlogit[3], "])"),
      paste0("Log: R=", Rlog[1], " (95%CI: [", Rlog[2], "-", Rlog[3], "])")
    ),
    col = Cols, cex = 0.9, box.lty = 0, lwd = 2, bg = "transparent"
  )
}
```

```{r simulate distribution several times and compare outcomes}
# Initialize a vector to store the maximum prevalence increase percentages
runs <- 1000
max_prevalence_increases <- numeric(runs)

for (i in 1:runs) {
  set.seed(i)
  
  # Generate normally distributed x with mean 190 and standard deviation 8.5
  n <- 427350
  x <- round(rnorm(n, mean = 190, sd = 8.5))
  
  # Calculate linear probabilities with a custom scaling to achieve ~4% overall probability of y = 1
  prob_y <- (x - min(x)) / (max(x) - min(x)) * 0.093
  
  # Sample y based on the calculated probabilities
  y <- rbinom(n, size = 1, prob = prob_y)
  
  # Create a data frame
  simulated_data <- data.frame(x = x, y = y, prob_y = prob_y)
  
  # Process the data to compute supplemental statistics
  supplemental_page_84 <- simulated_data %>%
    mutate(decile = ntile(x, 10)) %>%
    group_by(decile) %>%
    summarize(
      total_cases = sum(y),
      total_controls = length(y) - sum(y),
      Risk_Allels_mean = mean(x),
      Risk_Allels_sd = sd(x),
      prevalence = mean(y) * 100
    ) %>%
    mutate(
      allele_increase_perc = (Risk_Allels_mean / first(Risk_Allels_mean) - 1) * 100,
      prevalence_increase_perc = (prevalence / first(prevalence) - 1) * 100
    )
  
  # Record the maximum prevalence increase percentage for the current seed
  max_prevalence_increases[i] <- max(supplemental_page_84$prevalence_increase_perc)
}

# Convert the vector to a data frame for plotting
results_df <- data.frame(max_prevalence_increase = max_prevalence_increases)

# Plot the histogram of maximum prevalence increases
null_prevalence_increase <-
ggplot(results_df, aes(x = max_prevalence_increase)) +
  geom_histogram(binwidth = 5, color = "black", fill = "lightblue") +
  labs(
    x = "Maximum Prevalence Increase (%)",
    y = "Frequency",
    title = "Histogram of Maximum Prevalence Increases Across Simulations"
  ) +
  theme_minimal()

#ggsave("graphics/null_prev_increase.png", null_prevalence_increase)

max(results_df$max_prevalence_increase)
```

```{r Simulate again this time with allels - not finished yet}
set.seed(1)

# Parameters
n_cases <- 10000
n_vars <- 230
mean_sum <- 190
sd_sum <- 8.5

# Step 1: Generate the desired row-wise sums
desired_sums <- round(rnorm(n_cases, mean = mean_sum, sd = sd_sum))

# Initialize the data frame to hold binary variables
extended_data <- data.frame(matrix(0, nrow = n_cases, ncol = n_vars))
colnames(extended_data) <- paste0("V", 1:n_vars)
extended_data <- extended_data %>% mutate(across(everything(), as.integer))

# Step 2: Populate binary variables
for (i in 1:n_cases) {
  indices <- sample(seq_len(n_vars), desired_sums[i], replace = FALSE)
  extended_data[i, indices] <- 1
}

# Add the desired sums to the data frame for verification
extended_data$x <- rowSums(extended_data)

# Verification of the distribution
summary(extended_data$x)
hist(extended_data$x, breaks = 50, main = "Distribution of Case-wise Sums", xlab = "Sum")

# Display the first few rows of the data frame
head(extended_data)

# Calculate linear probabilities with a custom scaling to achieve ~4% overall probability of y = 1
prob_y <- (extended_data$case_sum - min(extended_data$case_sum)) / (max(extended_data$case_sum) - min(extended_data$case_sum)) * 0.093

# Sample y based on the calculated probabilities
y <- rbinom(n_cases, size = 1, prob = prob_y)

simulated_data <-
  cbind(extended_data,y, prob_y) %>% 
  select(x=case_sum,y,prob_y,everything())

```

```{r}
# Step 1: Calculate the sums for columns starting with "V"
summed_values <- simulated_data %>%
  select(starts_with("V")) %>%
  summarize(across(everything(), sum))

# Step 2: Transform the results into a long format
summed_values_long <- summed_values %>%
  pivot_longer(everything(), names_to = "variable", values_to = "sum_value")

# Step 3: Create the histogram
ggplot(summed_values_long, aes(x = sum_value)) +
  geom_histogram(binwidth = 15, color = "black", fill = "blue") +
  labs(title = "Histogram of Summed Values of Columns Starting with 'V'",
       x = "Sum of Values",
       y = "Frequency") +
  theme_minimal()
```

Prevalence and Odds ratios

```{r Plot OR and prevalence fo risk allels in paper against each other}
data.allele.freq <- read_delim("data/schunkert_risk_allels_cleaned.csv", col_names = c("SNP", "CHR", "BP", "A1", "OR", "P_VALUE", "Risk_allele_frequency_UKB"), delim = " ") %>% 
  mutate(
    CHR = as.numeric(CHR),
    BP = as.numeric(BP),
    OR = as.numeric(OR),
    P_VALUE = as.character(P_VALUE),
    Risk_allele_frequency_UKB = as.numeric(Risk_allele_frequency_UKB),
    Allele = 1
  )

ggplot(data.allele.freq, aes(x = Risk_allele_frequency_UKB, y = log(OR))) +
  geom_point() +
  labs(
    title = "OR vs. Risk Allele Frequency UKB",
    x = "Risk Allele Frequency UKB",
    y = "Odds Ratio (OR)"
  ) +
  theme_minimal()+
  theme(
    plot.background = element_rect(fill = "white"),  # Set the plot background to white
    panel.background = element_rect(fill = "white")
    )

#ggsave("graphics/or_vs_risk_allele_frequency.png")

# Perform the correlation test
cor_test_result <- cor.test(data.allele.freq$Risk_allele_frequency_UKB, log(data.allele.freq$OR), use = "complete.obs")

# Print the results
print(cor_test_result)

summary(data.allele.freq)

hist(data.allele.freq$Risk_allele_frequency_UKB)

hist(log(data.allele.freq$OR))

hist(data.allele.freq$OR)
```

```{r Simulate expected distribution of PRS based on prevalence of Risk Allels}
# Empirically: mean = 190; mean in first decile = 175.4, last decile = 205,2

prob_vector <-
rep(data.allele.freq$Risk_allele_frequency_UKB,2)

sum_vector <-
replicate(10000,sum(rbinom(length(prob_vector), size = 1, prob = prob_vector)))

hist(sum_vector)
summary(sum_vector)
sd(sum_vector)

sum_vector %>% 
  data.frame(sum_vector = .) %>% 
  mutate(decile = ntile(sum_vector,10)) %>% 
  group_by(decile) %>% 
  summarise(mean = mean(sum_vector))
```

Misc

```{r}
set.seed(123)

# Step 1: Generate 1000 probabilities from a normal distribution
n_vars <- 1000
n_cases <- 1000
mean_prob <- 0.5
sd_prob <- 0.2
probs <- rnorm(n_vars, mean = mean_prob, sd = sd_prob)

# Step 2: Scale probabilities to fall between 0.2% and 98%
scaled_probs <- pmin(pmax(probs, 0.002), 0.98)

# Step 3: Generate binary variables based on these probabilities
binary_matrix <- matrix(NA, nrow = n_cases, ncol = n_vars)

for (i in 1:n_vars) {
  binary_matrix[, i] <- rbinom(n_cases, size = 1, prob = scaled_probs[i])
}

# Step 4: Generate an additional binary variable with exactly 4.8% 1's
additional_binary_var <- rep(0, n_cases)
num_ones <- round(n_cases * 0.048)  # Calculate the number of 1's needed
one_positions <- sample(n_cases, num_ones)
additional_binary_var[one_positions] <- 1

# Append the additional binary variable to the matrix
binary_matrix <- cbind(binary_matrix, additional_binary_var)

# Verify the distribution of percentages of 1's
percent_ones <- colMeans(binary_matrix) * 100

# Plot the distribution of percentages of 1's
hist(percent_ones, breaks = 30, main = "Distribution of Percentages of 1's in Binary Variables", xlab = "Percentage of 1's")

# Check the percentage of 1's in the additional variable
additional_percentage <- mean(additional_binary_var) * 100
print(additional_percentage)  # Should be exactly 4.8


```

```{r}
# Number of binary variables
n_vars <- 100
n_cases <- 1000

# Simulate the binary variables
set.seed(123)
percentages <- pnorm(seq(-3, 3, length.out = n_vars), mean = 0, sd = 1)
binary_matrix <- matrix(0, nrow = n_cases, ncol = n_vars)

for (i in 1:n_vars) {
  binary_matrix[, i] <- rbinom(n_cases, 1, percentages[i])
}

# Add the additional binary variable with 4.8% 1s
additional_binary_var <- rep(0, n_cases)
additional_binary_var[sample(1:n_cases, 48)] <- 1

# Initialize a vector to store the odds ratios
odds_ratios <- numeric(n_vars)

# Calculate odds ratio for each binary variable
for (i in 1:n_vars) {
  # Create a contingency table
  contingency_table <- table(binary_matrix[, i], additional_binary_var)
  
  # Check for zero cells and handle accordingly
  if (all(contingency_table > 0)) {
    or_result <- oddsratio(contingency_table)
    odds_ratios[i] <- or_result$measure[2, "estimate"]
  } else {
    odds_ratios[i] <- NA  # Assign NA if the odds ratio cannot be calculated
  }
}

# Display the odds ratios
odds_ratios

# Optionally, plot the odds ratios
hist(odds_ratios, breaks = 30, main = "Distribution of Odds Ratios", xlab = "Odds Ratio")

```

